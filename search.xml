<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>《机器学习》第四章-决策树</title>
    <url>/zhangjunlong.github.io/2022/05/27/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8B%E7%AC%AC%E5%9B%9B%E7%AB%A0-%E5%86%B3%E7%AD%96%E6%A0%91/</url>
    <content><![CDATA[<h1 id="第四章-决策树"><a href="#第四章-决策树" class="headerlink" title="第四章 决策树"></a>第四章 决策树</h1><h2 id="4-1-决策树算法流程"><a href="#4-1-决策树算法流程" class="headerlink" title="4.1 决策树算法流程"></a>4.1 决策树算法流程</h2><p><img src="/zhangjunlong.github.io/zhangjunlong.github.io/2022/05/27/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8B%E7%AC%AC%E5%9B%9B%E7%AB%A0-%E5%86%B3%E7%AD%96%E6%A0%91/决策树算法.png" alt="决策树算法"></p>
<p>说明：</p>
<ul>
<li>第12行应该是continue？属性a*的每一个值都要生成子节点吧！</li>
<li>关键是第8行，从A中选择最优划分属性。</li>
</ul>
<h2 id="4-2-划分选择"><a href="#4-2-划分选择" class="headerlink" title="4.2 划分选择"></a>4.2 划分选择</h2><p>总目标：提高节点的纯度，本质上也反映了奥卡姆剃刀原则，即越简单的越好。</p>
<h3 id="4-2-1-方案一：基于信息增益"><a href="#4-2-1-方案一：基于信息增益" class="headerlink" title="4.2.1 方案一：基于信息增益"></a>4.2.1 方案一：基于信息增益</h3><h4 id="信息熵："><a href="#信息熵：" class="headerlink" title="信息熵："></a>信息熵：</h4><script type="math/tex; mode=display">
Ent(D)=-\sum_{k=1}^{|y|}p_klog_2p_k</script><p>其中，D代表当前样本，第k类样本所占的比例为$p_k$，若所有$p_k$都小于1，则$Ent(D)$取正数，假设只有一类样本，即至纯，那么信息熵为0，最小，因此也说明了信息熵越大则越杂乱越不纯。</p>
<h4 id="信息增益："><a href="#信息增益：" class="headerlink" title="信息增益："></a>信息增益：</h4><script type="math/tex; mode=display">
Gain(D,a) = Ent(D)-\sum_{v=1}^{V}\frac{|D^v|}{|D|}Ent(D^v)</script><p>其中，a表示用属性a对样本集D进行划分，假设属性a的取值为$a^1,a^2,…,a^V$，$D^v$表示样本D中属性a取$a^v$的样本。</p>
<p>说明：</p>
<ul>
<li>信息增益对可取值较多的属性有偏好，增益率可以将一定程度上消除这种偏好。</li>
</ul>
<h4 id="ID3决策树："><a href="#ID3决策树：" class="headerlink" title="ID3决策树："></a>ID3决策树：</h4><p>该决策树就是用最大化信息增益为准则选取最优划分属性，然后用<a href="#4-1">4.1</a>生成决策树。</p>
<p>ID3，Iterative Dichotomiser，迭代二分类器</p>
<h4 id="增益率："><a href="#增益率：" class="headerlink" title="增益率："></a>增益率：</h4><script type="math/tex; mode=display">
Gain\_ratio(D,a)=\frac{Gain(D,a)}{IV(a)}\\
\\
IV(a)=-\sum^V_{v=1}\frac{|D^v|}{|D|}log_2\frac{|D^v|}{|D|}</script><p>其中IV表示intrinsic value，固有值的意思，可取值较多时，IV值也较大，IV值直观上看是属性a划分的信息熵。</p>
<h4 id="C4-5决策树："><a href="#C4-5决策树：" class="headerlink" title="C4.5决策树："></a>C4.5决策树：</h4><p>该决策树利用了增益率来消除单纯信息增益带来的多取值属性的偏好，但也不是直接使用信息增益率，因为增益率毕竟只表示增益率，消除了增益本身的影响。</p>
<p>该决策树先从候选划分属性中找出信息增益高于平均的属性，再从总选择增益率最高的。</p>
<p>该决策树还进行了缺失值处理，<a href="#4-4-2">4.4.2</a></p>
<h4 id="基尼指数："><a href="#基尼指数：" class="headerlink" title="基尼指数："></a>基尼指数：</h4><script type="math/tex; mode=display">
\begin{align}
Gini(D)&=\sum_{k=1}^{|y|}\sum_{k'\neq k}p_kp_{k'}\\
&=1-\sum_{k=1}^{|y|}p_k^2
\end{align}</script><p>直观上看，Gini反映了从样本中随机抽取两个样本，其标签不一致的概率，因此Gini越小，样本集纯度越高。</p>
<p>类似信息增益，定义基尼指数：</p>
<script type="math/tex; mode=display">
Gini\_index(D,a)=\sum_{v=1}^V\frac{|D^v|}{|D|}Gini(D^v)</script><h4 id="CART决策树："><a href="#CART决策树：" class="headerlink" title="CART决策树："></a>CART决策树：</h4><p>该决策树就用了基尼指数作为选择最优划分属性的标注，然后用<a href="#4-1">4.1</a>的算法生成决策树。</p>
<h2 id="4-3-剪枝处理-pruning"><a href="#4-3-剪枝处理-pruning" class="headerlink" title="4.3 剪枝处理(pruning)"></a>4.3 剪枝处理(pruning)</h2><p>目的：划分过细导致过拟合，因此剪枝用来避免过拟合。</p>
<h3 id="4-3-1-预剪枝-prepruning"><a href="#4-3-1-预剪枝-prepruning" class="headerlink" title="4.3.1 预剪枝(prepruning):"></a>4.3.1 预剪枝(prepruning):</h3><p>一句话描述：正式生成分支之前，可以先使用测试集来测试一下生成分支之前和之后（将分支节点当作叶节点处理，标签用占比最大的标签）的性能，有提升则进行划分，否则不划分。</p>
<p>说明：</p>
<ul>
<li>优点：可以提高运算效率，节省计算资源，一定程度可以避免过拟合；</li>
<li>缺点：“试一下”的具体操作实际为贪心策略，无法保证继续划分下去之后性能一定下降。</li>
</ul>
<h3 id="4-3-2-后剪枝-postpruning"><a href="#4-3-2-后剪枝-postpruning" class="headerlink" title="4.3.2 后剪枝(postpruning)"></a>4.3.2 后剪枝(postpruning)</h3><p>一句话描述：在按照规则生成决策树后，按照节点生成的反向顺序，逐一判断节点取消分支是否能提高性能，直至根节点。</p>
<p>说明：</p>
<ul>
<li>优点：欠拟合风险小，泛化性能优于预剪枝（从过程上看似乎不那么贪心）。</li>
<li>缺点：慢。</li>
</ul>
<h2 id="4-4-连续值与缺失值"><a href="#4-4-连续值与缺失值" class="headerlink" title="4.4 连续值与缺失值"></a>4.4 连续值与缺失值</h2><h3 id="4-4-1-连续值"><a href="#4-4-1-连续值" class="headerlink" title="4.4.1 连续值"></a>4.4.1 连续值</h3><p>即连续属性而不是离散属性。</p>
<h4 id="基本策略——二分法"><a href="#基本策略——二分法" class="headerlink" title="基本策略——二分法:"></a>基本策略——二分法:</h4><p>将样本集中的连续属性按照从小到大的顺序排列，取一个截断点，将样本分为两类，这样就形成了分支节点。问题转化成了截断点如何选取，而这个问题同样可以用最大化信息增益来解决。</p>
<p>说明：</p>
<ul>
<li>与离散属性不同，连续属性可以重复使用。</li>
</ul>
<h3 id="4-4-2-缺失值"><a href="#4-4-2-缺失值" class="headerlink" title="4.4.2 缺失值"></a>4.4.2 缺失值</h3><p><span id="4-4-2"> </span>就是一些样本的属性数据不完整。面对这样样本集，有两个问题要解决：①如何选择最优属性；②如何对样本划分。</p>
<p>基本思路：赋予样本以权值。</p>
<p>给定训练集D和属性a，令$\widetilde{D}$表示D中在a上没有样本缺失的子集。</p>
<p>假定a有V个可取值，${a^1,a^2,…,a^V}$，令$\widetilde{D}^v$表$\widetilde{D}$中属性a取$a^v$的样本子集。</p>
<p>令$\widetilde{D}_k$表示$\widetilde{D}$中属于第k类的样本子集。</p>
<p>定义:</p>
<script type="math/tex; mode=display">
\begin{align}

\rho&=\frac{\sum_{\boldsymbol{x}\in\widetilde{D}}w_{\boldsymbol{x}}}{\sum_{\boldsymbol{x}\in D}w_{\boldsymbol{x}}}\\
\widetilde{p}_k&=\frac{\sum_{\boldsymbol{x}\in\widetilde{D}_k}w_{\boldsymbol{x}}}{\sum_{\boldsymbol{x}\in \widetilde{D}}w_{\boldsymbol{x}}},1\le k\le|y|\\
\widetilde{r}_v&=\frac{\sum_{\boldsymbol{x}\in\widetilde{D}^v}w_{\boldsymbol{x}}}{\sum_{\boldsymbol{x}\in \widetilde{D}}w_{\boldsymbol{x}}},1\le v\le V

\end{align}</script><h4 id="4-4-2-1-最优属性的选择"><a href="#4-4-2-1-最优属性的选择" class="headerlink" title="4.4.2.1 最优属性的选择"></a>4.4.2.1 最优属性的选择</h4><p>同样基于信息增益:</p>
<script type="math/tex; mode=display">
\begin{align}

Gain(D,a)&=\rho\times Gain(\widetilde{D},a)\\
&=\rho\times (Ent(\widetilde{D})-\sum_{v=1}^{V}\widetilde{r}_vEnt(\widetilde{D}^v))
\end{align}</script><p>其中</p>
<script type="math/tex; mode=display">
Ent(\widetilde{D}) = -\sum_{k=1}^{|y|}\widetilde{p}_klog_2\widetilde{p}_k</script><h4 id="4-4-2-2-样本划分"><a href="#4-4-2-2-样本划分" class="headerlink" title="4.4.2.2 样本划分"></a>4.4.2.2 样本划分</h4><p>对有缺失值的样本，则将样本划入所有子节点，且样本权值在属性值$a^v$对应的子节点中调整为$\widetilde{r}_v$。就是相当于以不同的概率划分到对应的属性值节点中，后续选择最优属性时，$\rho$值会变大。</p>
<h2 id="4-5-多变量决策树"><a href="#4-5-多变量决策树" class="headerlink" title="4.5 多变量决策树"></a>4.5 多变量决策树</h2><p>对决策树的思想理解很重要，是从几何的观点出发。</p>
<p>即决策树的每一个非叶节点，都对应坐标空间的一个或多个（分支≥3时）超平面。</p>
<p>因此有了“斜决策树”（oblique decision tree），每个非叶节点是一个形如$\sum_{i=1}^dw_ia_t=t$的线性分类器，其中权重和常数可以通过学习而得。</p>
<h3 id="4-5-1-典型多变量决策树"><a href="#4-5-1-典型多变量决策树" class="headerlink" title="4.5.1 典型多变量决策树"></a>4.5.1 典型多变量决策树</h3><p>OC1、感知机树</p>
<h2 id="4-6-阅读材料"><a href="#4-6-阅读材料" class="headerlink" title="4.6 阅读材料"></a>4.6 阅读材料</h2><ul>
<li>可以拓展阅读C4.5Rule。</li>
<li>用信息增益、增益率和基尼指数就够了，其它改进的程度不大。</li>
<li>多变量决策树可以和神经网络、感知机结合。</li>
<li>决策树的增量学习算法：ID4、ID5R、ITI，增量学习可以有效地降低新样本加入后的训练时间开销，适合工业化，但是多步增量学习后的模型会与基于全部数据训练而得的模型有较大差别。</li>
</ul>
]]></content>
      <categories>
        <category>技术</category>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>技术</tag>
      </tags>
  </entry>
</search>
